# -*- coding: utf-8 -*-
"""2.5- Analyse Dataset

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pfC0Q0vWN-0SBnOxOsOpaA67FunHHlA7
"""
# Import Libraries
import h5py
import numpy as np
import matplotlib
matplotlib.use('Agg')  # Use a non-interactive backend
import matplotlib.pyplot as plt
import pandas as pd
import torch
import os
from pathlib import Path

print("PyTorch version:", torch.__version__)
print("CUDA version:", torch.version.cuda)
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print(f"Using GPU: {torch.cuda.get_device_name(0)}")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# This take a few minutes so set to false if you don't need it
plot_correlations = False

feature_titles = {
    0: "Movement Vector Length",
    1: "Max Curvature Length",
    2: "Min Curvature Length",
    3: "Location X",
    4: "Location Y",
    5: "Location Z",
    6: "MVD-X",
    7: "MVD-Y",
    8: "MVD-Z",
    9: "MaCD-X",
    10: "MaCD-Y",
    11: "MaCD-Z",
    12: "MiCD-X",
    13: "MiCD-Y",
    14: "MiCD-Z",
    15: "No-X",
    16: "No-Y",
    17: "No-Z",
    18: "U-X",
    19: "U-Y",
    20: "U-Z",
    21: "V-X",
    22: "V-Y",
    23: "V-Z",
    25: "Angle"
}

feature_titles = {
    1: "Max Curvature Length",
    2: "Min Curvature Length",
    3: "MaCD-X",
    4: "MaCD-Y",
    5: "MaCD-Z",
    6: "MiCD-X",
    7: "MiCD-Y",
    8: "MiCD-Z",
    9: "No-X",
    10: "No-Y",
    11: "No-Z",
    12: "U-X",
    13: "U-Y",
    14: "U-Z",
    15: "V-X",
    16: "V-Y",
    17: "V-Z",
    25: "Angle"
}

dataset_name = "60-67"

# Get the script's directory
script_dir = Path(__file__).resolve().parent
project_root = script_dir.parent

# Base directory (datasets are parallel to the code folder)
base_dir = project_root.parent / "frustrated-composites-dataset"

# Set dataset files
features_file_path = base_dir / dataset_name / f"{dataset_name}_Merged_Features.h5"
labels_file_path = base_dir / dataset_name / f"{dataset_name}_Merged_Labels.h5"


# # Read Labels HDF5 File -fails with diverse widths
# with h5py.File(labels_file_path, 'r') as labels_file:
#     labels_train = labels_file['Labels/Train']
#     labels_test = labels_file['Labels/Test']
#
#     # Ensure all elements are at least 1D arrays
#     labels_train_data = [np.atleast_1d(labels_train[key][()]) for key in labels_train.keys()]
#     labels_test_data = [np.atleast_1d(labels_test[key][()]) for key in labels_test.keys()]
#
#     labels_train_flat = np.concatenate(labels_train_data)
#     labels_test_flat = np.concatenate(labels_test_data)
#
#     labels_all = np.concatenate((labels_train_flat, labels_test_flat))
#



# # Read Features HDF5 File
# with h5py.File(features_file_path, 'r') as features_file:
#     features_train = features_file['Features/Train']
#     features_test = features_file['Features/Test']
#
#     features_train_data = [features_train[key][()] for key in features_train.keys()]
#     features_test_data = [features_test[key][()] for key in features_test.keys()]
#
#     features_train_flat = np.concatenate(features_train_data)
#     features_test_flat = np.concatenate(features_test_data)
#
#     features_all = np.concatenate((features_train_flat, features_test_flat))

# Read Labels HDF5 File
with h5py.File(labels_file_path, 'r') as labels_file:
    labels_train = labels_file['Labels/Train']
    labels_test = labels_file['Labels/Test']

    # Find the max height and max width across all training and test labels
    max_height = max(max(labels_train[key].shape[0] for key in labels_train.keys()),
                     max(labels_test[key].shape[0] for key in labels_test.keys()))
    max_width = max(max(labels_train[key].shape[1] for key in labels_train.keys()),
                    max(labels_test[key].shape[1] for key in labels_test.keys()))

    print(f"labels max height {max_height} and max width {max_width}")



    # Function to pad labels dynamically
    def pad_label(label, max_h, max_w):
        label = np.atleast_3d(label)  # Ensure it's at least 3D (C, H, W)

        pad_h = max_h - label.shape[1] if label.shape[1] < max_h else 0
        pad_w = max_w - label.shape[2] if label.shape[2] < max_w else 0

        return np.pad(label, ((0, 0), (0, pad_h), (0, pad_w)), mode='constant')

    # Apply padding
    labels_train_data = [pad_label(labels_train[key][()], max_height, max_width) for key in labels_train.keys()]
    labels_test_data = [pad_label(labels_test[key][()], max_height, max_width) for key in labels_test.keys()]

    # Now concatenate safely
    labels_train_flat = np.concatenate(labels_train_data, axis=0)
    labels_test_flat = np.concatenate(labels_test_data, axis=0)
    labels_all = np.concatenate((labels_train_flat, labels_test_flat), axis=0)

# Read Features HDF5 File
with h5py.File(features_file_path, 'r') as features_file:
    features_train = features_file['Features/Train']
    features_test = features_file['Features/Test']

    # Find the max height and max width across all feature arrays
    max_height = max(max(features_train[key].shape[0] for key in features_train.keys()),
                     max(features_test[key].shape[0] for key in features_test.keys()))

    max_width = max(max(features_train[key].shape[1] for key in features_train.keys()),
                    max(features_test[key].shape[1] for key in features_test.keys()))

    print(f"features max height {max_height} and max width {max_width}")

    # Function to pad feature arrays dynamically
    def pad_feature(feature, max_h, max_w):
        feature = np.atleast_3d(feature)  # Ensure it's at least 3D (C, H, W)

        pad_h = max_h - feature.shape[1] if feature.shape[1] < max_h else 0
        pad_w = max_w - feature.shape[2] if feature.shape[2] < max_w else 0

        return np.pad(feature, ((0, 0), (0, pad_h), (0, pad_w)), mode='constant')

    # Apply padding to ensure uniform shape
    features_train_data = [pad_feature(features_train[key][()], max_height, max_width) for key in features_train.keys()]
    features_test_data = [pad_feature(features_test[key][()], max_height, max_width) for key in features_test.keys()]

    # Now concatenate safely
    features_train_flat = np.concatenate(features_train_data, axis=0)
    features_test_flat = np.concatenate(features_test_data, axis=0)
    features_all = np.concatenate((features_train_flat, features_test_flat), axis=0)

# Create a directory to save plots
plot_dir = "plots"
if not os.path.exists(plot_dir):
    os.makedirs(plot_dir)
scatter_plot_dir = "plots/correlations"
if not os.path.exists(scatter_plot_dir):
    os.makedirs(scatter_plot_dir)


# 1. Distribution of Labels
unique, counts = np.unique(labels_all, return_counts=True)
plt.bar(unique, counts)
plt.xlabel('Label')
plt.ylabel('Frequency')
plt.title('Distribution of Labels')
plt.savefig(os.path.join(plot_dir, 'distribution_labels.png'))
plt.close()

# 1. Distribution of Labels for Train Data
unique_train, counts_train = np.unique(labels_train_flat, return_counts=True)
plt.bar(unique_train, counts_train)
plt.xlabel('Label')
plt.ylabel('Frequency')
plt.title('Distribution of Train Labels')
plt.savefig(os.path.join(plot_dir, 'distribution_train_labels.png'))
plt.show()
plt.close()

# 1. Distribution of Labels for Test Data
unique_test, counts_test = np.unique(labels_test_flat, return_counts=True)
plt.bar(unique_test, counts_test)
plt.xlabel('Label')
plt.ylabel('Frequency')
plt.title('Distribution of Test Labels')
plt.savefig(os.path.join(plot_dir, 'distribution_test_labels.png'))
plt.show()
plt.close()

# 2. Statistics for Each Feature Column
features_combined = np.vstack(features_all)
column_stats = {
    'max': np.max(features_combined, axis=0),
    'min': np.min(features_combined, axis=0),
    'median': np.median(features_combined, axis=0),
    'mean': np.mean(features_combined, axis=0)
}

print("Column statistics:")
for stat, values in column_stats.items():
    print(f"{stat.capitalize()}: {values}")

# 3. Statistics for Each Feature Column in a Structured Format
column_stats_df = pd.DataFrame(column_stats)
print(column_stats_df)

# 2. Statistics for Each Feature Column in Train Data
features_train_combined = np.vstack(features_train_flat)
column_stats_train = {
    'max': np.max(features_train_combined, axis=0),
    'min': np.min(features_train_combined, axis=0),
    'median': np.median(features_train_combined, axis=0),
    'mean': np.mean(features_train_combined, axis=0)
}

# Statistics for Each Feature Column in a Structured Format
print("Train Column statistics:")
column_stats_df = pd.DataFrame(column_stats_train)
print(column_stats_df)

# 3. Statistics for Each Feature Column in Test Data
features_test_combined = np.vstack(features_test_flat)
column_stats_test = {
    'max': np.max(features_test_combined, axis=0),
    'min': np.min(features_test_combined, axis=0),
    'median': np.median(features_test_combined, axis=0),
    'mean': np.mean(features_test_combined, axis=0)
}

# Statistics for Each Feature Column in a Structured Format
print("Test Column statistics:")
column_stats_df = pd.DataFrame(column_stats_test)
print(column_stats_df)

# 4. Distribution of Feature Values
num_columns = features_combined.shape[1]
for i in range(num_columns):
    plt.hist(features_combined[:, i], bins=50, alpha=0.75, label=f'Column {i+1}')
    plt.xlabel('Feature Value')
    plt.ylabel('Frequency')
    plt.title(f'Distribution of Feature Values - Column {i+1}')

    plt.legend()
    plt.savefig(os.path.join(plot_dir, f'distribution_feature_values_column_{i+1}.png'))
    plt.close()


# 5. Histogram of Feature Samples per Range (Per Channel) with Bin Annotations
num_channels = features_combined.shape[1]
bins = 50  # Number of bins in the histogram

for i in range(num_channels):
    channel_data = features_combined[:, i]

    # Create histogram
    plt.figure(figsize=(20, 20))  # Enlarged plot size
    counts, bin_edges, _ = plt.hist(
        channel_data, bins=bins, alpha=0.75, color='blue', edgecolor='black'
    )
    plt.xlabel('Feature Value')
    plt.ylabel('Sample Count')
    plt.title(f'Histogram of Feature Samples - Channel {i}')
    plt.grid(axis='y', linestyle='--', alpha=0.7)

    # Annotate counts above each bin
    for count, bin_edge in zip(counts, bin_edges[:-1]):
        plt.text(
            bin_edge + (bin_edges[1] - bin_edges[0]) / 2,  # Center of the bin
            count + 0.5,  # Slightly above the bar
            str(int(count)),
            ha='center',
            va='bottom',
            fontsize=9,
            color='black'
        )

    # Save and close plot
    plt.savefig(os.path.join(plot_dir, f'histogram_feature_samples_channel_{i}.png'))
    plt.close()

# 4. Distribution of Feature Values for Train Data
num_columns_train = features_train_combined.shape[1]
for i in range(num_columns_train):
    plt.hist(features_train_combined[:, i], bins=50, alpha=0.75, label=f'Column {i+1}')
    plt.xlabel('Feature Value')
    plt.ylabel('Frequency')
    plt.title(f'Distribution of Train Feature Values - Column {i}')
    plt.legend()
    plt.savefig(os.path.join(plot_dir, f'distribution_train_feature_values_column_{i}.png'))
    plt.close()

# 4. Distribution of Feature Values for Test Data
num_columns_test = features_test_combined.shape[1]
for i in range(num_columns_test):
    plt.hist(features_test_combined[:, i], bins=50, alpha=0.75, label=f'Column {i+1}')
    plt.xlabel('Feature Value')
    plt.ylabel('Frequency')
    plt.title(f'Distribution of Test Feature Values - Column {i+1}')
    plt.legend()
    plt.savefig(os.path.join(plot_dir, f'distribution_test_feature_values_column_{i+1}.png'))
    plt.close()

# 5. Correlation plots

if plot_correlations:

    # 5.1. Number of features
    num_features = features_combined.shape[1]
    print(f"Number of features: {num_features}")

    # 5.2. Calculate the number of scatter plots
    num_feature_to_feature_plots = num_features * (num_features - 1) // 2  # nC2
    num_feature_to_label_plots = num_features  # One for each feature vs. labels
    total_plots = num_feature_to_feature_plots + num_feature_to_label_plots
    print(f"Total scatter plots needed: {total_plots}")

    # 5.3. Function to create and save scatter plots
    # Example of using the dictionary in the title generation
    def create_scatter_plot(x_data, y_data, x_index, y_index, save_name):
        if x_data is None or y_data is None or len(x_data) == 0 or len(y_data) == 0:
            print(f"Skipping plot for Feature {x_index} vs. Feature {y_index} due to invalid data.")
            return
        x_title = feature_titles.get(x_index, f"Feature {x_index + 1}")
        y_title = feature_titles.get(y_index, f"Feature {y_index + 1}")

        plt.figure(figsize=(15, 15))
        plt.scatter(x_data, y_data, alpha=0.5, s=1)
        plt.xlabel(x_title)
        plt.ylabel(y_title)
        plt.title(f"{x_title} vs. {y_title}")
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.savefig(save_name)
        plt.close()

    # 5.6. Generate scatter plots for feature-to-label correlations
    for i in range(num_features):
        x_data = features_combined[:, i]
        y_data = labels_all
        feature_title = feature_titles.get(i, f"Feature {i+1}")
        save_name = os.path.join(
            scatter_plot_dir,
            f"{feature_title.replace(' ', '_').replace('-', '_')}_vs_label.png"
        )
        create_scatter_plot(
            x_data, y_data,
            x_index=i,
            y_index=25,  # Labels don't have an index
            save_name=save_name
        )

    # 5.4. Generate scatter plots for feature-to-feature correlations
    for i in range(num_features):
        for j in range(i + 1, num_features):
            x_data = features_combined[:, i]
            y_data = features_combined[:, j]
            x_title = feature_titles.get(i, f"Feature {i+1}")
            y_title = feature_titles.get(j, f"Feature {j+1}")
            save_name = os.path.join(
                scatter_plot_dir,
                f"{x_title.replace(' ', '_').replace('-', '_')}_vs_{y_title.replace(' ', '_').replace('-', '_')}.png"
            )
            create_scatter_plot(
                x_data, y_data,
                x_index=i,
                y_index=j,
                save_name=save_name
            )

    print("Scatter plots for correlations have been generated and saved.")
else:
    print("not plotting correlations")


# 6. Compute mean and std per feature channel (from shape: [samples, spatial, channels])
features_tensor = torch.tensor(features_all, dtype=torch.float32)  # Shape: (N, S, C)

# Move channels to dim=0 → shape: (C, N, S)
features_tensor = features_tensor.permute(2, 0, 1)

# Compute stats per channel across all samples and spatial positions
channel_means = features_tensor.mean(dim=(1, 2)).numpy()
channel_stds = features_tensor.std(dim=(1, 2)).numpy()

print("\nDataset Feature Channel Means:")
print(channel_means.tolist())

print("\nDataset Feature Channel Standard Deviations:")
print(channel_stds.tolist())


# 7. Compute Mean and Covariance
print("\n#7 Computing Mahalanobis statistics...")

# Flatten each sample: (C, H, W) → (C*H*W)
flattened_features = [sample.reshape(-1) for sample in features_all]
X = np.stack(flattened_features, axis=0)  # shape: (N, D)

print(f"Flattened dataset shape for Mahalanobis: {X.shape}")

# Compute mean vector and covariance matrix
mean_vec = np.mean(X, axis=0)  # (D,)
cov_matrix = np.cov(X, rowvar=False)  # (D, D)

# Optional regularization to ensure invertibility
eps = 1e-5
cov_matrix += np.eye(cov_matrix.shape[0]) * eps

# Inverse covariance
inv_cov_matrix = np.linalg.inv(cov_matrix)

# Display for future use
print(f"Mean vector shape: {mean_vec.shape}")
print(f"Covariance matrix shape: {cov_matrix.shape}")
print(f"Inverse covariance matrix shape: {inv_cov_matrix.shape}")

# Save to plain text files (easy to open and inspect)
mean_path = os.path.join(plot_dir, "mahalanobis_mean_vec.txt")
inv_cov_path = os.path.join(plot_dir, "mahalanobis_inv_cov.txt")

np.savetxt(mean_path, mean_vec, delimiter=',', fmt='%.6f')
np.savetxt(inv_cov_path, inv_cov_matrix, delimiter=',', fmt='%.6f')

print(f"Saved Mahalanobis mean vector to: {mean_path}")
print(f"Saved inverse covariance matrix to: {inv_cov_path}")
