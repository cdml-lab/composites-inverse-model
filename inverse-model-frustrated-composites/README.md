# ğŸš§ WORK IN PROGRESS â€“ CONTENT NOT FINALIZED  ğŸš§

# ğŸ”¬ Inverse Model for Frustrated Composites

## ğŸ“– Overview
This project builds an inverse model for **frustrated composites** using deep learning. It consists of three main steps: **data preparation**, **model training**, and **prediction**. 
it supports two seperate options to get the inverse solution: 
1. train a direct inverse model and predict the fiber orientation of a surface.
2. train a forward model and then optimize to find the fiber orientation of a surface.


---

## ğŸ“‚ Project Structure
```
inverse-model-frustrated-composites/
â”‚â”€â”€ main/
â”‚   â”œâ”€â”€ prepare_dataset.py   # Prepares dataset for training
â”‚   â”œâ”€â”€ train_model.py       # Trains the ML model
â”‚   â”œâ”€â”€ predict_model.py     # Uses the trained model for predictions
â”‚â”€â”€ modules/                 # Helper modules for main files
â”‚   â”œâ”€â”€ s1_convert_excel_to_h5.py
â”‚   â”œâ”€â”€ s2_clean_and_reshape_h5.py
â”‚   â”œâ”€â”€ s3_merge_h5_files.py
â”‚â”€â”€ utils/                   # Utility scripts (optional tasks)
â”‚   â”œâ”€â”€ analyze_dataset.py
â”‚   â”œâ”€â”€ view_random_samples.py
â”‚â”€â”€ data/                    # Example datasets (link provided below)
â”‚â”€â”€ models/                  # Trained models
â”‚â”€â”€ results/                 # Output results
â”‚â”€â”€ README.md                # Documentation
â”‚â”€â”€ requirements.txt         # Dependencies
â”‚â”€â”€ config.yaml              # Project configuration file
```

---

## ğŸš€ Installation
### **1ï¸âƒ£ Clone the Repository**
```sh
git clone https://github.com/your-username/inverse-model-frustrated-composites.git
cd inverse-model-frustrated-composites
```

### **2ï¸âƒ£ Set Up a Virtual Environment**
```sh
python -m venv .venv
source .venv/bin/activate   # MacOS/Linux
# OR
.venv\Scripts\activate      # Windows
```

### **3ï¸âƒ£ Install Dependencies**
```sh
pip install -r requirements.txt
```

---

## ğŸ“‚ Example Dataset
The example dataset can be downloaded from OneDrive:  
ğŸ“‚ **[Download Example Dataset](https://onedrive.live.com/your-link-here)**

Once downloaded, place the dataset inside the `data/` folder.

---

## ğŸ”§ Changing Project Parameters
### **Dataset Normalization & Preprocessing**
Modify `config.yaml` to adjust **normalization settings**:
```yaml
normalization:
  method: "min-max"  # Options: "min-max", "z-score", "none"
  min: 0
  max: 1
```

### **Hyperparameter Tuning**
Modify the hyperparameters in `config.yaml`:
```yaml
hyperparameters:
  learning_rate: 0.001
  batch_size: 32
  epochs: 50
  optimizer: "adam"  # Options: "sgd", "adam"
```

### **W&B Integration**

https://wandb.ai/kapon-gal-technion/forward_model?nw=nwuserkapongal

Ensure W&B tracking is enabled by setting up your API key:
```sh
wandb login your-api-key
```
Modify `config.yaml` to log experiments:
```yaml
wandb:
  enable: true
  project: "frustrated-composites"
```



---

## ğŸ”§ Usage
### **1ï¸âƒ£ Prepare Dataset**
Convert `.xlsx` data to HDF5 format, clean, reshape, and merge.
```sh
python main/prep_dataset.py
```
Manually change the names and sizes of the datasets you wish to use in order to convert them from an excel to the appropriate formating. 
for example xlsx files named "Dataset_Input_82.xlsx" and "Dataset_Outut_82.xlsx" will be described as part of the dictionary as:
   "82": (y, x, n)
where y and x are the y and x size of the sheet and n is the number of channels you plan to use.
if you wish to change the type of data used from x,y,z to something else you need to change it in "modules/s2_clean_and_reshape_h5.py"
and change the variable "remove_split_columns" according to the columns you wish to keep.

list of columns collected by our GH dataset creation code:



### **2ï¸âƒ£ Train Model**
Train a deep learning model on the dataset.
```sh
python main/train_model.py
```

### **3ï¸âƒ£ Predict Results**
Use the trained model to make predictions.
```sh
python main/predict_model.py
```

---

## ğŸ“‚ File & Folder Descriptions
| **File/Folder** | **Description** |
|----------------|----------------|
| `main/prepare_dataset.py` | Converts & preprocesses dataset. |
| `main/train_model.py` | Trains the deep learning model. |
| `main/predict_model.py` | Runs the model to make predictions. |
| `modules/` | Contains helper scripts for processing. |
| `utils/` | Additional scripts for dataset analysis. |
| `data/` | Stores raw and processed datasets. |
| `models/` | Stores trained models. |
| `results/` | Saves experiment outputs. |

---

## ğŸ› ï¸ Dependencies
- **Python 3.8+**
- **Required Libraries:**
  ```sh
  pip install torch h5py pandas numpy openpyxl wandb
  ```

---

## ğŸ† Contributing
1. **Fork the repo** and clone it locally.
2. Create a **new branch** for your feature.
3. Commit your changes with a **descriptive message**.
4. Push your branch and create a **Pull Request**.

---

## ğŸ” Troubleshooting


---

## ğŸ“œ License


