# -*- coding: utf-8 -*-
"""6-Feature_Selection_and_Importance(RandomForest).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1guRxCSTMRUJ9IQ820AvZSpBeocYvOsK8

# Setup Google Drive Integration
"""


"""# Import Necessary Libraries"""

import h5py
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import torch
import joblib
import datetime






"""# Define Paths to the HDF5 Files"""

# Step 3: Define paths to the HDF5 files

og_dataset_name = '14'
dataset_name = '14_All_CNN'
patches = '_Patches'


num_rows, num_cols, num_depth = 5, 5, 15  # Adjusted to match the actual feature structure. The depth isn't used in the code at all so doesn't really need adjusting.


# Define the path and name for saving the model
save_model_path = "C:/Gal_Msc/Inverse Design Model/Inverse Model/saved_models"
current_date = datetime.datetime.now().strftime("%Y%m%d")
model_name = f"{dataset_name}{patches}_{current_date}.pkl"


features_file_path = "C:/Gal_Msc/Dataset/"+ og_dataset_name + '/' + dataset_name + '_Features' + patches + '.h5'
labels_file_path = "C:/Gal_Msc/Dataset/"+ og_dataset_name + '/' + dataset_name + '_Labels' + patches + '.h5'

#CUDA
"""# Read the HDF5 Files"""
print("PyTorch version:", torch.__version__)
print("CUDA version:", torch.version.cuda)
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print(f"Using GPU: {torch.cuda.get_device_name(0)}")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")




def read_hdf5_data(features_file, labels_file):
    def extract_data(file, group_name):
        data = []
        with h5py.File(file, 'r') as f:
            group = f[group_name]
            for key in group.keys():
                # Read the dataset within each group
                dataset = np.array(group[key])
                data.append(dataset)
        return np.array(data)

    X_train = extract_data(features_file, 'Features/Train')
    X_test = extract_data(features_file, 'Features/Test')
    y_train = extract_data(labels_file, 'Labels/Train')
    y_test = extract_data(labels_file, 'Labels/Test')

    # Reshape if necessary (flatten each sample)
    X_train = X_train.reshape(X_train.shape[0], -1, order='F')
    X_test = X_test.reshape(X_test.shape[0], -1, order='F')
    y_train = y_train.flatten()
    y_test = y_test.flatten()

    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = read_hdf5_data(features_file_path, labels_file_path)

# Print shapes of the datasets for debugging
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

"""# Filter Out Non-Numeric Data (Titles)"""

def filter_numeric_data(data):
    # Attempt to convert each element to float, filter out non-numeric values
    numeric_data = []
    for element in data:
        try:
            numeric_element = np.array(element, dtype=np.float32)
            numeric_data.append(numeric_element)
        except ValueError:
            # Skip non-numeric elements
            continue
    return np.array(numeric_data)

X_train_filtered = filter_numeric_data(X_train)
X_test_filtered = filter_numeric_data(X_test)
y_train_filtered = filter_numeric_data(y_train)
y_test_filtered = filter_numeric_data(y_test)

# Print shapes of the filtered datasets for debugging
print(f"X_train_filtered shape: {X_train_filtered.shape}")
print(f"X_test_filtered shape: {X_test_filtered.shape}")
print(f"y_train_filtered shape: {y_train_filtered.shape}")
print(f"y_test_filtered shape: {y_test_filtered.shape}")


"""# Ensure All Values are Numeric"""

# Ensure all values are numeric
X_train = X_train.astype(np.float32)
X_test = X_test.astype(np.float32)
y_train_filtered = y_train_filtered.astype(np.int32)
y_test_filtered = y_test_filtered.astype(np.int32)
print("final feature shape", X_train.shape)
print("final label shape", y_train_filtered.shape)

"""# Train Initial Model and Compute Feature Importance"""

rf = RandomForestClassifier(n_estimators=500, random_state=42)
rf.fit(X_train, y_train_filtered)


# Save the trained model
joblib.dump(rf, f"{save_model_path}/{model_name}")

# Compute feature importances
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

# Print feature importances
print("Feature importances:")
for f in range(X_train.shape[1]):
    print(f"Feature {indices[f]}: {importances[indices[f]]}")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Predict and evaluate the model
y_pred = rf.predict(X_test_filtered)
accuracy = accuracy_score(y_test_filtered, y_pred) # Use y_test_filtered here
print(f"Accuracy with all features: {accuracy}")

# Compute the confusion matrix
cm = confusion_matrix(y_test_filtered, y_pred) # And here

# Plot the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()

"""# Visualize Feature Importances"""

plt.figure()
plt.title("Feature Importances")
plt.bar(range(X_train.shape[1]), importances[indices], align="center")
plt.xticks(range(X_train.shape[1]), indices)
plt.xlim([-1, X_train.shape[1]])
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming X_train is shaped (num_samples, num_rows * num_cols)
num_samples, num_features = X_train.shape


# Map feature importances to (row, column) pairs
feature_importances = rf.feature_importances_.reshape(num_rows, num_cols)

# Sum importances for each row and column
row_importances = feature_importances.sum(axis=1)
column_importances = feature_importances.sum(axis=0)

# Visualize the importances
plt.figure(figsize=(12, 6))

# Row importances
plt.subplot(1, 2, 1)
sns.heatmap(row_importances.reshape(-1, 1), annot=True, cmap='viridis', cbar=False)
plt.title('Row Importances')
plt.xlabel('Importance')
plt.ylabel('Row Index')

# Column importances
plt.subplot(1, 2, 2)
sns.heatmap(column_importances.reshape(1, -1), annot=True, cmap='viridis', cbar=False)
plt.title('Column Importances')
plt.ylabel('Importance')
plt.xlabel('Column Index')

plt.tight_layout()
plt.show()

# Column names in the specified order
column_names = [
    "Movement Vector Length",
    "Max Curvature Length",
    "Min Curvature Length",
    "Location X",
    "Location Y",
    "Location Z",
    "MVD-X",
    "MVD-Y",
    "MVD-Z",
    "MaCD-X",
    "MaCD-Y",
    "MaCD-Z",
    "MiCD-X",
    "MiCD-Y",
    "MiCD-Z"
]

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Number of samples to display
num_samples = 20

# Ensure there are enough samples to select from
if num_samples > len(y_test):
    num_samples = len(y_test)

# Randomly select indices
random_indices = np.random.choice(len(y_test), num_samples, replace=False)

# Get the ground truth and predicted labels for the selected samples
selected_y_test = y_test[random_indices]
selected_y_pred = y_pred[random_indices]

# Plot the ground truth and predicted labels
fig, ax = plt.subplots(figsize=(10, 6))

# Create a table to display the ground truth and predicted labels
cell_text = []
for i in range(num_samples):
    cell_text.append([random_indices[i], selected_y_test[i], selected_y_pred[i]])

# Set up the table
table = ax.table(cellText=cell_text, colLabels=['Index', 'Ground Truth', 'Predicted'], cellLoc='center', loc='center')

# Remove the x and y axis
ax.axis('off')

# Adjust the table to fit the figure
table.auto_set_font_size(False)
table.set_fontsize(12)
table.scale(1.2, 1.2)

plt.title('Random 20 Samples: Ground Truth vs Predicted Labels')
plt.show()